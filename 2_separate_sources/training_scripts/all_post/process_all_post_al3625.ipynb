{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import nussl\n",
    "import torch\n",
    "from nussl.datasets import transforms as nussl_tfm\n",
    "from common import utils, argbind\n",
    "import matplotlib.pyplot as plt\n",
    "from nussl.ml.networks.modules import AmplitudeToDB, BatchNorm, RecurrentStack, Embedding\n",
    "from nussl.separation.base import MaskSeparationBase, DeepMixin, SeparationException\n",
    "from torch import nn\n",
    "# from torch.nn.utils import weight_norm\n",
    "from ignite.engine import Events, Engine, EventEnum\n",
    "from nussl.ml import SeparationModel\n",
    "from nussl.ml.networks.modules import (\n",
    "    Embedding, DualPath, DualPathBlock, STFT, Concatenate, \n",
    "    LearnedFilterBank, AmplitudeToDB, RecurrentStack,\n",
    "    MelProjection, BatchNorm, InstanceNorm, ShiftAndScale\n",
    ")\n",
    "from torch import optim\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from setup_al3625 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.logger()\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "name = os.getcwd().split(\"/\")[-1]\n",
    "\n",
    "eval_folder = \"../../eval_results\"\n",
    "output_folder = os.path.join(\"../../trained_models\", name)\n",
    "results_folder = os.path.join(eval_folder, name)\n",
    "separator_folder = os.path.join(\"../../trained_models\", name, \"separator\")\n",
    "\n",
    "saved_model_best = os.path.join(output_folder, \"checkpoints/best.model.pth\")\n",
    "saved_model_new = os.path.join(output_folder, \"checkpoints/latest.model.pth\")\n",
    "saved_opt_best = os.path.join(output_folder, \"checkpoints/best.optimizer.pth\")\n",
    "saved_opt_new = os.path.join(output_folder, \"checkpoints/latest.optimizer.pth\")\n",
    "saved_separator = os.path.join(separator_folder, \"separator.model.pth\")\n",
    "audio_folder = os.path.join(\"../../output_audio\", name)\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "if not os.path.exists(eval_folder):\n",
    "    os.mkdir(eval_folder)\n",
    "if not os.path.exists(results_folder):\n",
    "    os.mkdir(results_folder)\n",
    "if not os.path.exists(separator_folder):\n",
    "    os.mkdir(separator_folder)\n",
    "if not os.path.exists(audio_folder):\n",
    "    os.mkdir(audio_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50          \n",
    "BATCH_SIZE = 8 \n",
    "LEARNING_RATE = 1e-3 \n",
    "\n",
    "stft_params = nussl.STFTParams(window_length=512, hop_length=128)\n",
    "nf = stft_params.window_length // 2 + 1\n",
    "# corpus = get_corpus([full_train_folder, val_folder, test_folder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosteriorModel(nn.Module):\n",
    "    def __init__(self, num_features, num_audio_channels, hidden_size,\n",
    "                 num_layers, bidirectional, dropout, num_sources, \n",
    "                activation='sigmoid'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.recurrent_stack = RecurrentStack(\n",
    "            41*3, hidden_size, \n",
    "            num_layers, bool(bidirectional), dropout, 'lstm'\n",
    "        )\n",
    "\n",
    "        hidden_size = hidden_size * (int(bidirectional) + 1)\n",
    "        self.embedding = Embedding(num_features, hidden_size, \n",
    "                                   num_sources, activation, \n",
    "                                   num_audio_channels)\n",
    "        \n",
    "    def forward(self, mix_magnitude, posterior):\n",
    "\n",
    "        stack_data = self.recurrent_stack(posterior)\n",
    "        mask = self.embedding(stack_data)\n",
    "        estimates = mix_magnitude.unsqueeze(-1) * mask\n",
    "        \n",
    "        output = {\n",
    "            'mask': mask,\n",
    "            'estimates': estimates\n",
    "        }\n",
    "        return output\n",
    "        \n",
    "    @staticmethod\n",
    "    @argbind.bind_to_parser()\n",
    "    def build(num_features, num_audio_channels, hidden_size, \n",
    "              num_layers, bidirectional, dropout, num_sources, \n",
    "              activation='sigmoid'):\n",
    "        nussl.ml.register_module(PosteriorModel)\n",
    "        modules = {\n",
    "            'model': {\n",
    "                'class': 'PosteriorModel',\n",
    "                'args': {\n",
    "                    'num_features': num_features,\n",
    "                    'num_audio_channels': num_audio_channels,\n",
    "                    'hidden_size': hidden_size,\n",
    "                    'num_layers': num_layers,\n",
    "                    'bidirectional': bidirectional,\n",
    "                    'dropout': dropout,\n",
    "                    'num_sources': num_sources,\n",
    "                    'activation': activation\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        connections = [\n",
    "            ['model', ['mix_magnitude', 'posterior']]\n",
    "        ]\n",
    "        for key in ['mask', 'estimates']:\n",
    "            modules[key] = {'class': 'Alias'}\n",
    "            connections.append([key, [f'model:{key}']])\n",
    "        output = ['estimates', 'mask',]\n",
    "        config = {\n",
    "            'name': 'PosteriorModel',\n",
    "            'modules': modules,\n",
    "            'connections': connections,\n",
    "            'output': output\n",
    "        }\n",
    "        return nussl.ml.SeparationModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"posterior\"]\n",
    "post_depth=True\n",
    "use_corpus=False\n",
    "train_data, val_data, test_data = get_data(\"../../\", post_depth=post_depth, keys=keys, use_corpus=use_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PosteriorModel.build(num_features=nf,\n",
    "                            num_audio_channels=1, \n",
    "                            hidden_size=128,\n",
    "                            num_layers=3,\n",
    "                            bidirectional=True, \n",
    "                            dropout=0.3, \n",
    "                            num_sources=1, \n",
    "                            activation='sigmoid')\n",
    "\n",
    "if os.path.exists(saved_model_new):\n",
    "    model_checkpoint = torch.load(saved_model_new)\n",
    "    model = SeparationModel(model_checkpoint[\"config\"]) \n",
    "    model.load_state_dict(model_checkpoint[\"state_dict\"])\n",
    "    model.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    optimizer_checkpoint =  torch.load(saved_opt_new)\n",
    "    optimizer.load_state_dict(optimizer_checkpoint)\n",
    "\n",
    "else:\n",
    "    model = PosteriorModel.build(num_features=nf,\n",
    "                             num_audio_channels=1, \n",
    "                             hidden_size=512,\n",
    "                             num_layers=3,\n",
    "                             bidirectional=True, \n",
    "                             dropout=0.3, \n",
    "                             num_sources=1, \n",
    "                             activation='sigmoid')\n",
    "    model.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nussl.ml.train.loss.L1Loss()\n",
    "\n",
    "def train_step(engine, batch):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(batch) # forward pass\n",
    "    loss = loss_fn(\n",
    "        output['estimates'],\n",
    "        batch['source_magnitudes']\n",
    "    )\n",
    "    loss.backward() # backwards + gradient step\n",
    "    optimizer.step()\n",
    "    loss_vals = {\n",
    "        'L1Loss': loss.item(),\n",
    "        'loss': loss.item()\n",
    "    }\n",
    "    return loss_vals\n",
    "\n",
    "def val_step(engine, batch):\n",
    "    with torch.no_grad():\n",
    "        output = model(data=batch) # forward pass\n",
    "    loss = loss_fn(\n",
    "        output['estimates'],\n",
    "        batch['source_magnitudes']\n",
    "    )\n",
    "    loss_vals = {\n",
    "        'L1Loss': loss.item(),\n",
    "        'loss': loss.item()\n",
    "    }\n",
    "    return loss_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_data, num_workers=4, batch_size=BATCH_SIZE)\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_data, num_workers=4, batch_size=BATCH_SIZE) \n",
    "\n",
    "trainer, validator = modified_create_train_and_validation_engines(\n",
    "    train_step, val_step, device=DEVICE\n",
    ")\n",
    "nussl.ml.train.add_stdout_handler(trainer, validator)\n",
    "nussl.ml.train.add_validate_and_checkpoint(output_folder, model,\n",
    "    optimizer, train_data, trainer, val_dataloader, validator)\n",
    "nussl.ml.train.add_progress_bar_handler(trainer, validator)\n",
    "\n",
    "if os.path.exists(saved_model_new):\n",
    "    trainer.load_state_dict(model_checkpoint[\"metadata\"][\"trainer.state_dict\"])\n",
    "    trainer.state.epoch_history = model_checkpoint[\"metadata\"][\"trainer.state.epoch_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/09/2023 04:24:12 PM | engine.py:876 Engine run resuming from iteration 750, epoch 10 until 50 epochs\n",
      "05/09/2023 04:24:58 PM | engine.py:1086 Current run is terminating due to exception: CUDA out of memory. Tried to allocate 4.28 GiB (GPU 0; 11.17 GiB total capacity; 2.51 GiB already allocated; 1.83 GiB free; 2.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "05/09/2023 04:25:09 PM | engine.py:992 Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 4.28 GiB (GPU 0; 11.17 GiB total capacity; 2.51 GiB already allocated; 1.83 GiB free; 2.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.28 GiB (GPU 0; 11.17 GiB total capacity; 2.51 GiB already allocated; 1.83 GiB free; 2.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m     plt\u001b[39m.\u001b[39mtight_layout()\n\u001b[1;32m     18\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m---> 20\u001b[0m trainer\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m     21\u001b[0m     train_dataloader,\n\u001b[1;32m     22\u001b[0m     max_epochs \u001b[39m=\u001b[39;49m EPOCHS   \n\u001b[1;32m     23\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/sep/lib/python3.8/site-packages/ignite/engine/engine.py:892\u001b[0m, in \u001b[0;36mEngine.run\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    889\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mdataloader \u001b[39m=\u001b[39m data\n\u001b[1;32m    891\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterrupt_resume_enabled:\n\u001b[0;32m--> 892\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_internal_run()\n\u001b[1;32m    893\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    894\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_run_legacy()\n",
      "File \u001b[0;32m~/miniconda3/envs/sep/lib/python3.8/site-packages/ignite/engine/engine.py:935\u001b[0m, in \u001b[0;36mEngine._internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_run_generator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_run_as_gen()\n\u001b[1;32m    934\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 935\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_internal_run_generator)\n\u001b[1;32m    936\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m out:\n\u001b[1;32m    937\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_run_generator \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sep/lib/python3.8/site-packages/ignite/engine/engine.py:993\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataloader_iter \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    992\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEngine run is terminating due to exception: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 993\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_exception(e)\n\u001b[1;32m    995\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataloader_iter \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\n",
      "File \u001b[0;32m~/miniconda3/envs/sep/lib/python3.8/site-packages/ignite/engine/engine.py:638\u001b[0m, in \u001b[0;36mEngine._handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fire_event(Events\u001b[39m.\u001b[39mEXCEPTION_RAISED, e)\n\u001b[1;32m    637\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 638\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/sep/lib/python3.8/site-packages/ignite/engine/engine.py:959\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataloader_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setup_engine()\n\u001b[0;32m--> 959\u001b[0m epoch_time_taken \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_once_on_dataset_as_gen()\n\u001b[1;32m    961\u001b[0m \u001b[39m# time is available for handlers but must be updated after fire\u001b[39;00m\n\u001b[1;32m    962\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mtimes[Events\u001b[39m.\u001b[39mEPOCH_COMPLETED\u001b[39m.\u001b[39mname] \u001b[39m=\u001b[39m epoch_time_taken\n",
      "File \u001b[0;32m~/miniconda3/envs/sep/lib/python3.8/site-packages/ignite/engine/engine.py:1087\u001b[0m, in \u001b[0;36mEngine._run_once_on_dataset_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCurrent run is terminating due to exception: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1087\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_exception(e)\n\u001b[1;32m   1089\u001b[0m \u001b[39mreturn\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/miniconda3/envs/sep/lib/python3.8/site-packages/ignite/engine/engine.py:638\u001b[0m, in \u001b[0;36mEngine._handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fire_event(Events\u001b[39m.\u001b[39mEXCEPTION_RAISED, e)\n\u001b[1;32m    637\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 638\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/sep/lib/python3.8/site-packages/ignite/engine/engine.py:1068\u001b[0m, in \u001b[0;36mEngine._run_once_on_dataset_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fire_event(Events\u001b[39m.\u001b[39mITERATION_STARTED)\n\u001b[1;32m   1066\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_terminate_or_interrupt()\n\u001b[0;32m-> 1068\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39moutput \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_function(\u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate\u001b[39m.\u001b[39;49mbatch)\n\u001b[1;32m   1069\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fire_event(Events\u001b[39m.\u001b[39mITERATION_COMPLETED)\n\u001b[1;32m   1070\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_terminate_or_interrupt()\n",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_step\u001b[39m(engine, batch):\n\u001b[1;32m      4\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> 5\u001b[0m     output \u001b[39m=\u001b[39m model(batch) \u001b[39m# forward pass\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(\n\u001b[1;32m      7\u001b[0m         output[\u001b[39m'\u001b[39m\u001b[39mestimates\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m      8\u001b[0m         batch[\u001b[39m'\u001b[39m\u001b[39msource_magnitudes\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m     loss\u001b[39m.\u001b[39mbackward() \u001b[39m# backwards + gradient step\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sep/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/sep/lib/python3.8/site-packages/nussl/ml/networks/separation_model.py:181\u001b[0m, in \u001b[0;36mSeparationModel.forward\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m             input_data\u001b[39m.\u001b[39mappend(output[c] \u001b[39mif\u001b[39;00m c \u001b[39min\u001b[39;00m output \u001b[39melse\u001b[39;00m data[c])\n\u001b[0;32m--> 181\u001b[0m _output \u001b[39m=\u001b[39m layer(\u001b[39m*\u001b[39;49minput_data, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    182\u001b[0m added_keys \u001b[39m=\u001b[39m []\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(_output, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/sep/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m, in \u001b[0;36mPosteriorModel.forward\u001b[0;34m(self, mix_magnitude, posterior)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, mix_magnitude, posterior):\n\u001b[0;32m---> 19\u001b[0m     stack_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrecurrent_stack(posterior)\n\u001b[1;32m     20\u001b[0m     mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(stack_data)\n\u001b[1;32m     21\u001b[0m     estimates \u001b[39m=\u001b[39m mix_magnitude\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m mask\n",
      "File \u001b[0;32m~/miniconda3/envs/sep/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/sep/lib/python3.8/site-packages/nussl/ml/networks/modules/blocks.py:574\u001b[0m, in \u001b[0;36mRecurrentStack.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    572\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mreshape(shape[\u001b[39m0\u001b[39m], shape[\u001b[39m1\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    573\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mflatten_parameters()\n\u001b[0;32m--> 574\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(data)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    575\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/sep/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/sep/lib/python3.8/site-packages/torch/nn/modules/rnn.py:774\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    773\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    775\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    776\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    777\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    778\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.28 GiB (GPU 0; 11.17 GiB total capacity; 2.51 GiB already allocated; 1.83 GiB free; 2.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training(engine):\n",
    "    plt.plot(engine.state.iter_history['loss'])\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    data = engine.state.epoch_history\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.subplot(111)\n",
    "    plt.plot(data['validation/L1Loss'], label='val')\n",
    "    plt.plot(data['train/L1Loss'], label='train')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "trainer.run(\n",
    "    train_dataloader,\n",
    "    max_epochs = EPOCHS   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separator = DeepMaskEstimationPosterior(\n",
    "    nussl.AudioSignal(), None, model_path=saved_model_best,\n",
    "    device=DEVICE\n",
    ")\n",
    "separator.model.save(saved_separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(test_data):\n",
    "    separator.audio_signal = item['mix']\n",
    "    separator.posterior = item['posterior']\n",
    "    item['posterior'].to(DEVICE)\n",
    "    estimates = separator()\n",
    "\n",
    "    source_keys = list(item['sources'].keys())\n",
    "    estimates = {\n",
    "        'vocals': estimates[0],\n",
    "        'non-vocals': item['mix'] - estimates[0]\n",
    "    }\n",
    "\n",
    "    sources = [item['sources'][k] for k in source_keys]\n",
    "    estimates = [estimates[k] for k in source_keys]\n",
    "\n",
    "    evaluator = nussl.evaluation.BSSEvalScale(\n",
    "        sources, estimates, source_labels=source_keys\n",
    "    )\n",
    "    scores = evaluator.evaluate()\n",
    "    output_file = os.path.join(eval_folder, f\"{i}.json\")\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(scores, f, indent=4)\n",
    "    if i % 5 == 0:\n",
    "        print([i], output_file)\n",
    "\n",
    "    song_dir = os.path.join(audio_folder, str(i))\n",
    "    if not os.path.exists(song_dir):\n",
    "        os.mkdir(song_dir)\n",
    "    estimates[0].write_audio_to_file(os.path.join(song_dir, \"Predicted Vocals.wav\"))\n",
    "    estimates[1].write_audio_to_file(os.path.join(song_dir, \"Predicted Non-Vocals.wav\"))\n",
    "    sources[0].write_audio_to_file(os.path.join(song_dir, \"True Vocals.wav\"))\n",
    "    sources[1].write_audio_to_file(os.path.join(song_dir, \"True Non-Vocals.wav\"))\n",
    "    item[\"mix\"].write_audio_to_file(os.path.join(song_dir, \"Full Mix.wav\"))\n",
    "    \n",
    "    if i > 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = test_data[4]\n",
    "separator.audio_signal = item['mix']\n",
    "separator.posterior = item['posterior']\n",
    "estimates = separator()\n",
    "estimates.append(item['mix'] - estimates[0])\n",
    "visualize_masks(estimates)\n",
    "\n",
    "\n",
    "source_keys = list(item['sources'].keys())\n",
    "sources = [item['sources'][k] for k in source_keys]\n",
    "visualize_masks(sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "997c4558be07356c2b0a8bb58c51598cd0966f4bdac088ce10c0dada3ab32118"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
